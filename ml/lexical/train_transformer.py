"""
Fine-tune DistilBERT for Sarcasm Detection

This script fine-tunes a DistilBERT model to achieve higher accuracy than TF-IDF (~80% â†’ ~88-92%).
DistilBERT is 40% smaller and 60% faster than BERT while retaining 97% of its performance.

Requirements:
    pip install torch transformers datasets accelerate scikit-learn

Usage:
    python train_transformer.py                    # Train with defaults
    python train_transformer.py --epochs 5        # More epochs
    python train_transformer.py --export-onnx     # Export to ONNX after training

Training time:
    - GPU (RTX 3080): ~5-10 minutes
    - CPU: ~1-2 hours (not recommended)
    - Apple M1/M2: ~15-30 minutes (MPS acceleration)
"""

import argparse
import json
import urllib.request
from pathlib import Path

import torch
from torch.utils.data import DataLoader, Dataset
from transformers import (
    DistilBertTokenizer,
    DistilBertForSequenceClassification,
    AdamW,
    get_linear_schedule_with_warmup,
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, f1_score
from tqdm import tqdm

# =============================================================================
# Configuration
# =============================================================================

SCRIPT_DIR = Path(__file__).parent
DATA_DIR = SCRIPT_DIR / "data"
LOCAL_DATA_PATH = DATA_DIR / "sarcasm.json"
REMOTE_DATA_URL = "https://storage.googleapis.com/learning-datasets/sarcasm.json"
MODEL_OUTPUT_DIR = SCRIPT_DIR / "distilbert_sarcasm"
BACKEND_DIR = SCRIPT_DIR.parent.parent / "backend"

# Model config
MODEL_NAME = "distilbert-base-uncased"
MAX_LENGTH = 128  # Max tokens (headlines are short)
BATCH_SIZE = 32
LEARNING_RATE = 2e-5
EPOCHS = 3
WARMUP_RATIO = 0.1
RANDOM_STATE = 42

# =============================================================================
# Dataset
# =============================================================================


class SarcasmDataset(Dataset):
    """PyTorch Dataset for sarcasm detection."""

    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long),
        }


def load_dataset():
    """Load sarcasm dataset with local cache fallback."""
    # Check local cache first
    if LOCAL_DATA_PATH.exists():
        print(f"ğŸ“ Loading from local cache: {LOCAL_DATA_PATH}")
        with open(LOCAL_DATA_PATH, "r") as f:
            data = json.load(f)
    else:
        print(f"ğŸ“¥ Downloading dataset from {REMOTE_DATA_URL}...")
        DATA_DIR.mkdir(parents=True, exist_ok=True)
        urllib.request.urlretrieve(REMOTE_DATA_URL, LOCAL_DATA_PATH)
        with open(LOCAL_DATA_PATH, "r") as f:
            data = json.load(f)
        print(f"   âœ“ Cached to {LOCAL_DATA_PATH}")

    texts = [item["headline"] for item in data]
    labels = [item["is_sarcastic"] for item in data]

    return texts, labels


def add_conversational_examples(texts, labels):
    """Add conversational sarcasm examples to improve real-world performance."""

    # Sarcastic examples
    sarcastic = [
        "Oh great, another Monday",
        "Oh great, another meeting that could have been an email",
        "Oh wonderful, my coffee is cold",
        "Oh fantastic, my bus is late",
        "Oh perfect, it's raining and I forgot my umbrella",
        "Yeah right, like that's going to happen",
        "Sure, because that makes total sense",
        "I just LOVE waiting in lines",
        "I LOVE when people are late",
        "Wow, you're SO smart",
        "Wow, thanks for letting me know at the last minute",
        "Wow, what a surprise",
        "Shocking. Nobody could have predicted that",
        "Well that was unexpected. Not.",
        "Color me surprised",
        "Best day ever, clearly",
        "Living the dream over here",
        "Thanks for nothing",
        "Thanks a lot, really appreciate it",
        "And I'm the Queen of England",
        "Sure, and pigs fly",
    ]

    # Non-sarcastic examples
    non_sarcastic = [
        "Thank you so much for your help",
        "I really appreciate you taking the time",
        "Thanks for letting me know",
        "This is actually really helpful",
        "Great job on the presentation",
        "The weather is nice today",
        "The meeting is at 3pm",
        "I'll send you the email",
        "How was your weekend",
        "Good morning everyone",
        "Have a great weekend",
        "I can't wait for the vacation",
        "Looking forward to the concert",
        "The system is working properly now",
        "We're making good progress",
    ]

    # Add examples once â€” avoid exact duplication to prevent overfitting on specific phrases
    texts.extend(sarcastic)
    labels.extend([1] * len(sarcastic))
    texts.extend(non_sarcastic)
    labels.extend([0] * len(non_sarcastic))

    return texts, labels


# =============================================================================
# Training
# =============================================================================


def get_device():
    """Get the best available device."""
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸš€ Using CUDA: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("ğŸ Using Apple MPS (Metal Performance Shaders)")
    else:
        device = torch.device("cpu")
        print("ğŸ’» Using CPU (training will be slow)")
    return device


def train_epoch(model, dataloader, optimizer, scheduler, device):
    """Train for one epoch."""
    model.train()
    total_loss = 0
    predictions = []
    actuals = []

    progress = tqdm(dataloader, desc="Training")
    for batch in progress:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids, attention_mask=attention_mask, labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        preds = torch.argmax(outputs.logits, dim=1)
        predictions.extend(preds.cpu().numpy())
        actuals.extend(labels.cpu().numpy())

        progress.set_postfix({"loss": loss.item()})

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(actuals, predictions)

    return avg_loss, accuracy


def evaluate(model, dataloader, device):
    """Evaluate the model."""
    model.eval()
    total_loss = 0
    predictions = []
    actuals = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            outputs = model(
                input_ids=input_ids, attention_mask=attention_mask, labels=labels
            )

            total_loss += outputs.loss.item()
            preds = torch.argmax(outputs.logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            actuals.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(actuals, predictions)
    f1 = f1_score(actuals, predictions, average="weighted")

    return avg_loss, accuracy, f1, predictions, actuals


def export_to_onnx(model, tokenizer, output_path):
    """Export model to ONNX format for faster inference."""
    print("\nğŸ“¦ Exporting to ONNX...")

    model.eval()
    model.to("cpu")

    # Dummy input for tracing
    dummy_text = "This is a sample sentence"
    dummy_input = tokenizer(
        dummy_text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=MAX_LENGTH,
    )

    torch.onnx.export(
        model,
        (dummy_input["input_ids"], dummy_input["attention_mask"]),
        output_path,
        input_names=["input_ids", "attention_mask"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch_size"},
            "attention_mask": {0: "batch_size"},
            "logits": {0: "batch_size"},
        },
        opset_version=14,
    )

    print(f"   âœ“ Saved ONNX model to {output_path}")
    print(f"   Size: {output_path.stat().st_size / 1024 / 1024:.1f} MB")


# =============================================================================
# Main
# =============================================================================


def main():
    parser = argparse.ArgumentParser(
        description="Fine-tune DistilBERT for sarcasm detection"
    )
    parser.add_argument(
        "--epochs", type=int, default=EPOCHS, help="Number of training epochs"
    )
    parser.add_argument("--batch-size", type=int, default=BATCH_SIZE, help="Batch size")
    parser.add_argument("--lr", type=float, default=LEARNING_RATE, help="Learning rate")
    parser.add_argument(
        "--export-onnx", action="store_true", help="Export to ONNX after training"
    )
    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ¯ SARCASM DETECTION - DISTILBERT FINE-TUNING")
    print("=" * 60)

    # Device
    device = get_device()

    # Load data
    print("\nğŸ“Š Loading Dataset...")
    texts, labels = load_dataset()
    texts, labels = add_conversational_examples(texts, labels)

    print(f"   Total samples: {len(texts):,}")
    print(f"   Sarcastic: {sum(labels):,} ({sum(labels)/len(labels)*100:.1f}%)")

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.15, random_state=RANDOM_STATE, stratify=labels
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.1, random_state=RANDOM_STATE, stratify=y_train
    )

    print(f"   Train: {len(X_train):,} | Val: {len(X_val):,} | Test: {len(X_test):,}")

    # Load tokenizer and model
    print(f"\nğŸ¤– Loading {MODEL_NAME}...")
    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)
    model = DistilBertForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=2,
        id2label={0: "not_sarcastic", 1: "sarcastic"},
        label2id={"not_sarcastic": 0, "sarcastic": 1},
    )
    model.to(device)

    # Create datasets
    train_dataset = SarcasmDataset(X_train, y_train, tokenizer, MAX_LENGTH)
    val_dataset = SarcasmDataset(X_val, y_val, tokenizer, MAX_LENGTH)
    test_dataset = SarcasmDataset(X_test, y_test, tokenizer, MAX_LENGTH)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)

    # Optimizer and scheduler
    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=0.01)
    total_steps = len(train_loader) * args.epochs
    warmup_steps = int(total_steps * WARMUP_RATIO)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps
    )

    # Training loop
    print(f"\nğŸ“ Training for {args.epochs} epochs...")
    print("=" * 60)

    best_val_accuracy = 0
    best_model_state = None

    for epoch in range(args.epochs):
        print(f"\nğŸ“… Epoch {epoch + 1}/{args.epochs}")

        # Train
        train_loss, train_acc = train_epoch(
            model, train_loader, optimizer, scheduler, device
        )
        print(f"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")

        # Validate
        val_loss, val_acc, val_f1, _, _ = evaluate(model, val_loader, device)
        print(
            f"   Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}"
        )

        # Save best model
        if val_acc > best_val_accuracy:
            best_val_accuracy = val_acc
            best_model_state = model.state_dict().copy()
            print(f"   âœ“ New best model! (Val Acc: {val_acc:.4f})")

    # Load best model
    if best_model_state:
        model.load_state_dict(best_model_state)

    # Final evaluation
    print("\n" + "=" * 60)
    print("ğŸ“Š FINAL EVALUATION")
    print("=" * 60)

    test_loss, test_acc, test_f1, predictions, actuals = evaluate(
        model, test_loader, device
    )

    print("\nğŸ¯ Test Results:")
    print(f"   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
    print(f"   F1 Score: {test_f1:.4f}")

    print("\n   Classification Report:")
    print(
        classification_report(
            actuals, predictions, target_names=["Not Sarcastic", "Sarcastic"]
        )
    )

    # Save model
    print("\n" + "=" * 60)
    print("ğŸ’¾ SAVING MODEL")
    print("=" * 60)

    MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    model.save_pretrained(MODEL_OUTPUT_DIR)
    tokenizer.save_pretrained(MODEL_OUTPUT_DIR)
    print(f"   âœ“ Saved to {MODEL_OUTPUT_DIR}")

    # Calculate model size
    total_size = sum(
        f.stat().st_size for f in MODEL_OUTPUT_DIR.glob("*") if f.is_file()
    )
    print(f"   Total size: {total_size / 1024 / 1024:.1f} MB")

    # Export to ONNX if requested
    if args.export_onnx:
        onnx_path = MODEL_OUTPUT_DIR / "model.onnx"
        export_to_onnx(model, tokenizer, onnx_path)

    # Test predictions
    print("\n" + "=" * 60)
    print("ğŸ§ª SAMPLE PREDICTIONS")
    print("=" * 60)

    test_sentences = [
        ("Oh great, another meeting that could have been an email", True),
        ("Wow, you're SO smart", True),
        ("Yeah right, like that's ever going to happen", True),
        ("I just LOVE waiting in traffic", True),
        ("Thank you for helping me with this", False),
        ("The weather is nice today", False),
        ("Have a great weekend", False),
        ("Scientists discover water is wet", True),
    ]

    model.eval()
    print("\nğŸ“ Predictions:")

    correct = 0
    for text, expected in test_sentences:
        inputs = tokenizer(
            text, return_tensors="pt", truncation=True, max_length=MAX_LENGTH
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)
            prob_sarcastic = probs[0][1].item()

        predicted = prob_sarcastic > 0.5
        is_correct = predicted == expected
        correct += is_correct

        emoji = "ğŸ˜" if predicted else "ğŸ˜"
        marker = "âœ“" if is_correct else "âœ—"

        print(f"   {marker} {emoji} [{prob_sarcastic:.3f}] '{text[:50]}'")

    print(f"\n   Sample accuracy: {correct}/{len(test_sentences)}")

    print("\n" + "=" * 60)
    print("âœ… TRAINING COMPLETE!")
    print("=" * 60)
    print(f"\nModel saved to: {MODEL_OUTPUT_DIR}")
    print(f"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
    print("\nTo use this model in the backend, update the loader to use transformers.")


if __name__ == "__main__":
    main()
